{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "import gensim.downloader as api\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>Cleaned_Content</th>\n",
       "      <th>Preprocessed_content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elders react to Gaga Five Foot Two</td>\n",
       "      <td>elders react to gaga five foot two</td>\n",
       "      <td>elder react gaga foot</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey Anna, could you do longer cameo in Ant-Man...</td>\n",
       "      <td>hey anna could you do longer cameo in antman s...</td>\n",
       "      <td>hey anna long cameo antman sequel need act</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is it me or does that railing not seem very hi...</td>\n",
       "      <td>is it me or does that railing not seem very hi...</td>\n",
       "      <td>rail high</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think she might be a scammer who's trying to...</td>\n",
       "      <td>i think she might be a scammer whos trying to ...</td>\n",
       "      <td>think scammer s try money lawsuit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I like how he gives everyone the ol' Heil Hitler.</td>\n",
       "      <td>i like how he gives everyone the ol heil hitler</td>\n",
       "      <td>like give ol heil hitler</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>This video was awful, Iâm sorry Katy. Your p...</td>\n",
       "      <td>this video was awful im sorry katy your past v...</td>\n",
       "      <td>video awful m sorry katy past video sicken wtf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>The house caught fire from their fire musicð...</td>\n",
       "      <td>the house caught fire from their fire music</td>\n",
       "      <td>house catch fire fire music</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>your lips make me want to eat powdered donuts ...</td>\n",
       "      <td>your lips make me want to eat powdered donuts</td>\n",
       "      <td>lip want eat powdered donut</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>Yrs later andy in jail 4 abusing daughter.</td>\n",
       "      <td>yrs later andy in jail abusing daughter</td>\n",
       "      <td>yrs later andy jail abuse daughter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>https://www.gofundme.com/4k55174everyone like ...</td>\n",
       "      <td>httpswwwgofundmecomkeveryone like this i need ...</td>\n",
       "      <td>httpswwwgofundmecomkeveryone like need car cen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>883 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          comment_text  \\\n",
       "0                   Elders react to Gaga Five Foot Two   \n",
       "1    Hey Anna, could you do longer cameo in Ant-Man...   \n",
       "2    Is it me or does that railing not seem very hi...   \n",
       "3    I think she might be a scammer who's trying to...   \n",
       "4    I like how he gives everyone the ol' Heil Hitler.   \n",
       "..                                                 ...   \n",
       "878  This video was awful, Iâm sorry Katy. Your p...   \n",
       "879  The house caught fire from their fire musicð...   \n",
       "880  your lips make me want to eat powdered donuts ...   \n",
       "881         Yrs later andy in jail 4 abusing daughter.   \n",
       "882  https://www.gofundme.com/4k55174everyone like ...   \n",
       "\n",
       "                                       Cleaned_Content  \\\n",
       "0                   elders react to gaga five foot two   \n",
       "1    hey anna could you do longer cameo in antman s...   \n",
       "2    is it me or does that railing not seem very hi...   \n",
       "3    i think she might be a scammer whos trying to ...   \n",
       "4      i like how he gives everyone the ol heil hitler   \n",
       "..                                                 ...   \n",
       "878  this video was awful im sorry katy your past v...   \n",
       "879        the house caught fire from their fire music   \n",
       "880      your lips make me want to eat powdered donuts   \n",
       "881            yrs later andy in jail abusing daughter   \n",
       "882  httpswwwgofundmecomkeveryone like this i need ...   \n",
       "\n",
       "                                  Preprocessed_content  Label  \n",
       "0                                elder react gaga foot      0  \n",
       "1           hey anna long cameo antman sequel need act      0  \n",
       "2                                            rail high      0  \n",
       "3                    think scammer s try money lawsuit      0  \n",
       "4                             like give ol heil hitler      0  \n",
       "..                                                 ...    ...  \n",
       "878     video awful m sorry katy past video sicken wtf      0  \n",
       "879                        house catch fire fire music      0  \n",
       "880                        lip want eat powdered donut      0  \n",
       "881                 yrs later andy jail abuse daughter      0  \n",
       "882  httpswwwgofundmecomkeveryone like need car cen...      0  \n",
       "\n",
       "[883 rows x 4 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"YouTube_Datasets/preprocessed_youtube.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "883"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_speech_words = [\n",
    "    'hate', 'violence', 'abuse', 'offend', 'discrimination', 'racism', 'oppression', 'bigot', 'ignorant', \n",
    "    'supremacist', 'feminazi', 'cunt', 'slut', 'whore', 'bitch', 'skank', 'thot', 'ho', 'gold digger', \n",
    "    'motherfucker', 'cockroach', 'asshole', 'dickhead', 'prick', 'scumbag', 'bastard', 'fuck', 'nigga', \n",
    "    'nigger', 'chink', 'gook', 'spic', 'wetback', 'cracker', 'honky', 'kike', 'wog', 'towelhead', \n",
    "    'sandnigger', 'faggot', 'fag', 'dyke', 'tranny', 'homo', 'sissy', 'fairy', 'pansy', 'sodomite', \n",
    "    'infidel', 'heathen', 'crusader', 'raghead', 'jihadist', 'taliban', 'beaner', 'gypsy', 'paki', \n",
    "    'wop', 'dago', 'mick', 'jap', 'yid', 'kaffir', 'cholo', 'zebra', 'white trash', 'redneck', 'hillbilly',\n",
    "    'karen', 'snowflake', 'beta', 'incel', 'simp', 'boomer', 'npc', 'soyboy', 'mgtow', 'swine', 'trumpist', \n",
    "    'libtard', 'nazi', 'commie', 'sjw', 'ratchet', 'ghetto', 'redpill', 'based', 'chad', 'beta male', \n",
    "    'alpha male', 'cuck', 'cock', 'broke bitch', 'pussy ass', 'turd', 'feminist', 'terrorist', 'islamist'\n",
    "]\n",
    "\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "def detect_hate_speech(text):\n",
    "    return sum([1 for word in hate_speech_words if word in text.lower()])\n",
    "\n",
    "def pos_tagging(text):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    return pos_tags\n",
    "\n",
    "def get_word2vec_embedding(text):\n",
    "    tokens = [token.text.lower() for token in nlp(text)]\n",
    "    word_vectors = [word2vec_model[word] for word in tokens if word in word2vec_model]\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    tokens = [token.text for token in nlp(text)]\n",
    "    return len(set(tokens)) / len(tokens) if len(tokens) > 0 else 0\n",
    "\n",
    "def sentence_complexity(text):\n",
    "    return len(re.findall(r'\\.', text))\n",
    "\n",
    "def get_sbert_embedding(text):\n",
    "    return sbert_model.encode(text, convert_to_numpy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Preprocessed_content'] = data['Preprocessed_content'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0 = data[data['Label'] == 0].sample(n=600, random_state=42)\n",
    "class_1 = data[data['Label'] == 1].sample(n=76, random_state=42)\n",
    "data = pd.concat([class_0, class_1])\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis done\n",
      "Hate speech detection done\n",
      "POS tagging detection done\n",
      "Word2Vec embedding done\n",
      "SBERT embedding done\n",
      "Lexical diversity done\n",
      "Sentence complexity done\n"
     ]
    }
   ],
   "source": [
    "data['sentiment'] = data['Preprocessed_content'].apply(sentiment_analysis)\n",
    "print('Sentiment analysis done')\n",
    "\n",
    "data['hate_speech_count'] = data['Preprocessed_content'].apply(detect_hate_speech)\n",
    "print('Hate speech detection done')\n",
    "\n",
    "data['pos_tags'] = data['Preprocessed_content'].apply(pos_tagging)\n",
    "print('POS tagging detection done')\n",
    "\n",
    "data['word2vec'] = data['Preprocessed_content'].apply(get_word2vec_embedding)\n",
    "print('Word2Vec embedding done')\n",
    "\n",
    "data['sbert_embedding'] = data['Preprocessed_content'].apply(get_sbert_embedding)\n",
    "print('SBERT embedding done')\n",
    "\n",
    "data['lexical_diversity'] = data['Preprocessed_content'].apply(lexical_diversity)\n",
    "print('Lexical diversity done')\n",
    "\n",
    "data['sentence_complexity'] = data['Preprocessed_content'].apply(sentence_complexity)\n",
    "print('Sentence complexity done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>Cleaned_Content</th>\n",
       "      <th>Preprocessed_content</th>\n",
       "      <th>Label</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>hate_speech_count</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>word2vec</th>\n",
       "      <th>sbert_embedding</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>sentence_complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Click this link to sign up for $300 http://for...</td>\n",
       "      <td>click this link to sign up for httpformoneyonl...</td>\n",
       "      <td>click link sign httpformoneyonlycomrefer</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>[VERB, PROPN, NOUN, PROPN]</td>\n",
       "      <td>[-0.074788414, 0.022460938, -0.17325847, 0.134...</td>\n",
       "      <td>[-0.04272761, -0.061473675, -0.019803623, 0.00...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Have you tried turning it off and on?</td>\n",
       "      <td>have you tried turning it off and on</td>\n",
       "      <td>try turn</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>[VERB, NOUN]</td>\n",
       "      <td>[0.08862305, 0.15405273, 0.078308105, 0.145263...</td>\n",
       "      <td>[0.02100541, -0.040340815, -0.03201399, -0.004...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If u saying Jordan one of the best u ainât i...</td>\n",
       "      <td>if u saying jordan one of the best u aint in t...</td>\n",
       "      <td>u say jordan good u be not good ur statement s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>1</td>\n",
       "      <td>[PRON, VERB, PROPN, PROPN, PROPN, VERB, PART, ...</td>\n",
       "      <td>[-0.049316406, 0.03329827, 0.05125517, 0.07881...</td>\n",
       "      <td>[0.064440705, 0.04145527, -0.045367703, 0.0012...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unexpected turn of events: dude is Asian</td>\n",
       "      <td>unexpected turn of events dude is asian</td>\n",
       "      <td>unexpected turn event dude asian</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADJ, NOUN, NOUN, VERB, PROPN]</td>\n",
       "      <td>[-0.020629883, 0.0014404297, 0.018041993, 0.16...</td>\n",
       "      <td>[-0.037429538, 0.09296043, 0.030556811, 0.0632...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Made me cry...and that's a good thing...God bless</td>\n",
       "      <td>made me cryand thats a good thinggod bless</td>\n",
       "      <td>cryand s good thinggod bless</td>\n",
       "      <td>0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0</td>\n",
       "      <td>[PROPN, PART, ADJ, PROPN, VERB]</td>\n",
       "      <td>[-0.021647135, 0.14908855, -0.023803711, 0.171...</td>\n",
       "      <td>[-0.15125263, 0.063120745, 0.071638584, -0.044...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>11:43 those are stripper heels..is Â Cristine ...</td>\n",
       "      <td>those are stripper heelsis cristine is a retir...</td>\n",
       "      <td>stripper heelsis cristine retire hoe</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>[ADJ, NOUN, NOUN, VERB, NOUN]</td>\n",
       "      <td>[0.052083332, -0.10514323, -0.038950603, 0.192...</td>\n",
       "      <td>[-0.06953118, 0.054789927, -0.025015866, -0.07...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>What a shit weak freestyle\\n\\nhttps://youtu.be...</td>\n",
       "      <td>what a shit weak freestylennhttpsyoutubeclslnl...</td>\n",
       "      <td>shit weak freestylennhttpsyoutubeclslnlzaati a...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.287500</td>\n",
       "      <td>0</td>\n",
       "      <td>[PROPN, ADJ, PROPN, INTJ, NOUN, VERB, PROPN, P...</td>\n",
       "      <td>[0.026530266, -0.0042800903, -0.0074310303, 0....</td>\n",
       "      <td>[0.03838325, -0.107888035, 0.05925496, -0.0388...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>Ohh black nigaa give away legit? Lol</td>\n",
       "      <td>ohh black nigaa give away legit lol</td>\n",
       "      <td>ohh black nigaa away legit lol</td>\n",
       "      <td>1</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADJ, ADJ, NOUN, ADV, NOUN, NOUN]</td>\n",
       "      <td>[-0.049121093, 0.016284179, 0.105603024, 0.153...</td>\n",
       "      <td>[-0.13155076, 0.07625024, -0.10333354, 0.04866...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>Y'all are a bunch of racist. When you like som...</td>\n",
       "      <td>yall are a bunch of racist when you like someo...</td>\n",
       "      <td>you bunch racist like label racist</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>[PRON, VERB, NOUN, ADP, NOUN, NOUN]</td>\n",
       "      <td>[0.017089844, 0.018656412, 0.119010925, 0.1214...</td>\n",
       "      <td>[0.014129858, 0.05866641, -0.13190544, 0.02975...</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>The first person to die from a *L I C C*</td>\n",
       "      <td>the first person to die from a l i c c</td>\n",
       "      <td>person die l c c</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>[NOUN, VERB, NOUN, NOUN, NUM]</td>\n",
       "      <td>[-0.013549805, 0.04550781, 0.010131836, 0.1735...</td>\n",
       "      <td>[0.018447237, 0.015107418, -0.012942691, -0.02...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>676 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          comment_text  \\\n",
       "0    Click this link to sign up for $300 http://for...   \n",
       "1                Have you tried turning it off and on?   \n",
       "2    If u saying Jordan one of the best u ainât i...   \n",
       "3             Unexpected turn of events: dude is Asian   \n",
       "4    Made me cry...and that's a good thing...God bless   \n",
       "..                                                 ...   \n",
       "671  11:43 those are stripper heels..is Â Cristine ...   \n",
       "672  What a shit weak freestyle\\n\\nhttps://youtu.be...   \n",
       "673               Ohh black nigaa give away legit? Lol   \n",
       "674  Y'all are a bunch of racist. When you like som...   \n",
       "675           The first person to die from a *L I C C*   \n",
       "\n",
       "                                       Cleaned_Content  \\\n",
       "0    click this link to sign up for httpformoneyonl...   \n",
       "1                 have you tried turning it off and on   \n",
       "2    if u saying jordan one of the best u aint in t...   \n",
       "3              unexpected turn of events dude is asian   \n",
       "4           made me cryand thats a good thinggod bless   \n",
       "..                                                 ...   \n",
       "671  those are stripper heelsis cristine is a retir...   \n",
       "672  what a shit weak freestylennhttpsyoutubeclslnl...   \n",
       "673                ohh black nigaa give away legit lol   \n",
       "674  yall are a bunch of racist when you like someo...   \n",
       "675             the first person to die from a l i c c   \n",
       "\n",
       "                                  Preprocessed_content  Label  sentiment  \\\n",
       "0             click link sign httpformoneyonlycomrefer      0   0.000000   \n",
       "1                                             try turn      0   0.000000   \n",
       "2    u say jordan good u be not good ur statement s...      0   0.183333   \n",
       "3                     unexpected turn event dude asian      0   0.050000   \n",
       "4                         cryand s good thinggod bless      0   0.700000   \n",
       "..                                                 ...    ...        ...   \n",
       "671               stripper heelsis cristine retire hoe      1   0.000000   \n",
       "672  shit weak freestylennhttpsyoutubeclslnlzaati a...      1  -0.287500   \n",
       "673                     ohh black nigaa away legit lol      1   0.316667   \n",
       "674                 you bunch racist like label racist      1   0.000000   \n",
       "675                                   person die l c c      1   0.000000   \n",
       "\n",
       "     hate_speech_count                                           pos_tags  \\\n",
       "0                    0                         [VERB, PROPN, NOUN, PROPN]   \n",
       "1                    0                                       [VERB, NOUN]   \n",
       "2                    1  [PRON, VERB, PROPN, PROPN, PROPN, VERB, PART, ...   \n",
       "3                    0                     [ADJ, NOUN, NOUN, VERB, PROPN]   \n",
       "4                    0                    [PROPN, PART, ADJ, PROPN, VERB]   \n",
       "..                 ...                                                ...   \n",
       "671                  1                      [ADJ, NOUN, NOUN, VERB, NOUN]   \n",
       "672                  0  [PROPN, ADJ, PROPN, INTJ, NOUN, VERB, PROPN, P...   \n",
       "673                  0                  [ADJ, ADJ, NOUN, ADV, NOUN, NOUN]   \n",
       "674                  0                [PRON, VERB, NOUN, ADP, NOUN, NOUN]   \n",
       "675                  0                      [NOUN, VERB, NOUN, NOUN, NUM]   \n",
       "\n",
       "                                              word2vec  \\\n",
       "0    [-0.074788414, 0.022460938, -0.17325847, 0.134...   \n",
       "1    [0.08862305, 0.15405273, 0.078308105, 0.145263...   \n",
       "2    [-0.049316406, 0.03329827, 0.05125517, 0.07881...   \n",
       "3    [-0.020629883, 0.0014404297, 0.018041993, 0.16...   \n",
       "4    [-0.021647135, 0.14908855, -0.023803711, 0.171...   \n",
       "..                                                 ...   \n",
       "671  [0.052083332, -0.10514323, -0.038950603, 0.192...   \n",
       "672  [0.026530266, -0.0042800903, -0.0074310303, 0....   \n",
       "673  [-0.049121093, 0.016284179, 0.105603024, 0.153...   \n",
       "674  [0.017089844, 0.018656412, 0.119010925, 0.1214...   \n",
       "675  [-0.013549805, 0.04550781, 0.010131836, 0.1735...   \n",
       "\n",
       "                                       sbert_embedding  lexical_diversity  \\\n",
       "0    [-0.04272761, -0.061473675, -0.019803623, 0.00...           1.000000   \n",
       "1    [0.02100541, -0.040340815, -0.03201399, -0.004...           1.000000   \n",
       "2    [0.064440705, 0.04145527, -0.045367703, 0.0012...           0.888889   \n",
       "3    [-0.037429538, 0.09296043, 0.030556811, 0.0632...           1.000000   \n",
       "4    [-0.15125263, 0.063120745, 0.071638584, -0.044...           1.000000   \n",
       "..                                                 ...                ...   \n",
       "671  [-0.06953118, 0.054789927, -0.025015866, -0.07...           1.000000   \n",
       "672  [0.03838325, -0.107888035, 0.05925496, -0.0388...           1.000000   \n",
       "673  [-0.13155076, 0.07625024, -0.10333354, 0.04866...           1.000000   \n",
       "674  [0.014129858, 0.05866641, -0.13190544, 0.02975...           0.833333   \n",
       "675  [0.018447237, 0.015107418, -0.012942691, -0.02...           0.800000   \n",
       "\n",
       "     sentence_complexity  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      0  \n",
       "..                   ...  \n",
       "671                    0  \n",
       "672                    0  \n",
       "673                    0  \n",
       "674                    0  \n",
       "675                    0  \n",
       "\n",
       "[676 rows x 11 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dataset with extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"YouTube_Datasets/features_youtube.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
